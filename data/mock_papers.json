[
  {
    "paper_id": "RP-001",
    "title": "ChronoRAG: Time-Aware Retrieval for Living Literature Reviews",
    "year": 2023,
    "language": "en",
    "venue": "ACL Findings",
    "authors": ["Mina Patel", "Arjun Rao"],
    "abstract": "ChronoRAG studies whether retrieval order matters for literature review agents. The paper introduces a time-aware retriever that prioritizes recent evidence while preserving seminal citations.",
    "methodology": "The authors benchmark 14,000 paper snippets across machine learning and medicine. They compare dense retrieval, BM25, and a weighted hybrid retriever in iterative review loops.",
    "findings": "Hybrid retrieval improved factual grounding by 11% versus dense-only retrieval and reduced stale-citation errors in long reviews. Gains were strongest when the query required chronology.",
    "limitations": "The benchmark used mostly English sources and lacked robust multilingual relevance judgments.",
    "citations": ["RP-006", "RP-008"],
    "keywords": ["hybrid retrieval", "chronology", "literature review", "factual grounding"],
    "stance": "supports"
  },
  {
    "paper_id": "RP-002",
    "title": "Failure Modes of Dense Retrieval in Biomedical QA Pipelines",
    "year": 2024,
    "language": "en",
    "venue": "EMNLP",
    "authors": ["Lucia Gomez", "Ethan Cho", "R. Ivanov"],
    "abstract": "This work audits dense retrievers used in biomedical evidence synthesis. The core claim is that dense-only setups overfit popular entities and miss minority trial outcomes.",
    "methodology": "Researchers build a stress suite with contradictory trial abstracts and rare disease terms. They evaluate retrieval recall, contradiction exposure, and downstream summary stability.",
    "findings": "Dense retrieval alone misses contradictory outcomes in 27% of cases, while hybrid retrieval recovers minority evidence and improves contradiction detection.",
    "limitations": "The contradiction labels were weakly supervised and may include annotation noise.",
    "citations": ["RP-001", "RP-009"],
    "keywords": ["biomedical QA", "dense retrieval", "contradiction detection", "minority evidence"],
    "stance": "supports"
  },
  {
    "paper_id": "RP-003",
    "title": "跨语言引文检索用于科研综述代理",
    "year": 2024,
    "language": "zh",
    "venue": "Chinese NLP Conference",
    "authors": ["张晨", "王一航"],
    "abstract": "本文研究中英双语文献综述中的检索偏差。结果显示，单语检索会系统性遗漏中文方法论文。",
    "methodology": "作者构建了含中英文摘要、方法、结论的结构化语料，并在多轮 agent 规划-检索-重规划框架下测试混合检索。",
    "findings": "混合检索在跨语言召回率上提升了18%，并显著降低了引用幻觉。对于包含术语歧义的问题，重规划步骤是关键。",
    "limitations": "数据集覆盖的学科较窄，主要来自 NLP 与医学信息学。",
    "citations": ["RP-006", "RP-010"],
    "keywords": ["跨语言", "混合检索", "引用幻觉", "重规划"],
    "stance": "supports"
  },
  {
    "paper_id": "RP-004",
    "title": "Benchmark Instability in Long-Context Scientific Summarization",
    "year": 2025,
    "language": "en",
    "venue": "NAACL",
    "authors": ["Adele Kim", "Jonas Fischer"],
    "abstract": "The paper argues that reported gains in long-context review agents are sensitive to benchmark split choices. Some gains disappear after de-duplicating citation neighborhoods.",
    "methodology": "Authors re-run six public baselines with strict de-duplication and temporal holdout splits. They include a targeted probe for citation leakage.",
    "findings": "Several retrieval-heavy systems lose 6-9 F1 after leakage control. Hybrid systems remain competitive but confidence calibration becomes unstable.",
    "limitations": "The study does not include non-English corpora and focuses on machine learning venues.",
    "citations": ["RP-001", "RP-006"],
    "keywords": ["benchmark stability", "long context", "citation leakage", "evaluation"],
    "stance": "challenges"
  },
  {
    "paper_id": "RP-005",
    "title": "Retrieval-Free Scientific Reasoning with Synthetic Memory",
    "year": 2025,
    "language": "en",
    "venue": "ICLR Workshop",
    "authors": ["Nora Singh", "Q. Li"],
    "abstract": "This paper claims retrieval can be replaced by synthetic memory distillation for narrow-domain reviews. The authors present a memory bank trained on expert summaries.",
    "methodology": "The model is tuned on repeated review tasks in one subdomain. Performance is measured on speed, citation precision, and update latency after new papers appear.",
    "findings": "Synthetic memory is faster but degrades sharply under unseen terminology. Citation precision drops when conflicting fresh evidence arrives.",
    "limitations": "Generalization outside the training niche is poor, and the memory bank requires expensive curation.",
    "citations": ["RP-004"],
    "keywords": ["retrieval-free", "synthetic memory", "citation precision", "domain shift"],
    "stance": "challenges"
  },
  {
    "paper_id": "RP-006",
    "title": "MetaReview-H: Hybrid Retrieval for Transparent Literature Synthesis",
    "year": 2024,
    "language": "en",
    "venue": "TACL",
    "authors": ["Samuel Green", "Priya Natarajan", "Lei Xu"],
    "abstract": "MetaReview-H introduces a transparent review agent with explicit tool traces and citation-grounded claims. It uses keyword and semantic channels jointly.",
    "methodology": "The study compares retrieval strategies under noisy PDF extraction, multilingual abstracts, and contradictory source injections. Human reviewers audit trace quality.",
    "findings": "Hybrid retrieval plus replanning outperforms single-shot pipelines on evidence completeness and reviewer trust. The largest gains come from ambiguity-triggered replans.",
    "limitations": "Manual reviewer audits are expensive and difficult to reproduce at scale.",
    "citations": ["RP-008", "RP-009", "RP-010"],
    "keywords": ["hybrid retrieval", "tool trace", "ambiguity", "replanning", "trust"],
    "stance": "supports"
  },
  {
    "paper_id": "RP-007",
    "title": "Revisión Multilingüe con Agentes: evidencia conflictiva en salud pública",
    "year": 2025,
    "language": "es",
    "venue": "IberNLP",
    "authors": ["Carla Benitez", "Luis Herrera"],
    "abstract": "Este trabajo analiza agentes de revisión en español e inglés con fuentes conflictivas. Señala que la detección de desacuerdos depende de recuperar estudios minoritarios.",
    "methodology": "Se construye un conjunto con artículos de salud pública, notas de política y réplicas. El sistema compara recuperación híbrida contra BM25 en bucles iterativos.",
    "findings": "La recuperación híbrida mejora cobertura de evidencia y reduce resúmenes sesgados. Sin embargo, la calidad cae cuando el parser PDF omite tablas críticas.",
    "limitations": "El estudio usa pocos dominios y tiene anotaciones limitadas para matices pragmáticos.",
    "citations": ["RP-002", "RP-006"],
    "keywords": ["multilingüe", "evidencia conflictiva", "salud pública", "agentes"],
    "stance": "supports"
  },
  {
    "paper_id": "RP-008",
    "title": "PDF Noise Taxonomy for Scientific Agent Pipelines",
    "year": 2023,
    "language": "en",
    "venue": "LREC",
    "authors": ["Helen Park", "Yuji Tanaka"],
    "abstract": "This paper introduces a taxonomy of PDF extraction errors affecting retrieval pipelines. Missing table headers and broken references are dominant failure classes.",
    "methodology": "The dataset contains synthetically corrupted and real-world PDF parses across 9 fields. They measure retrieval recall drop and citation mismatch rates.",
    "findings": "Hybrid retrieval is more robust than dense-only retrieval but still fails when citation anchors are malformed. Error-aware parsing recovers up to 13% recall.",
    "limitations": "Corruption templates may not capture all publisher-specific formatting artifacts.",
    "citations": ["RP-010"],
    "keywords": ["PDF parsing", "noise taxonomy", "citation anchors", "robustness"],
    "stance": "mixed"
  },
  {
    "paper_id": "RP-009",
    "title": "Citation Hallucination Under Partial Context Windows",
    "year": 2025,
    "language": "en",
    "venue": "COLING",
    "authors": ["Fatima El-Sayed", "Bruno Keller"],
    "abstract": "The paper quantifies citation hallucination when review agents operate with compressed context windows. Hallucination spikes when retrieval fails to surface negative evidence.",
    "methodology": "Authors evaluate chunking policies, memory compression schemes, and retrieval depth. They inspect whether agents can recover by replanning after ambiguity signals.",
    "findings": "Aggressive memory compression increases hallucinated citation chains. Replanning mitigates some failures but requires diverse retrieval candidates.",
    "limitations": "Experiments rely on automatic hallucination detectors that may undercount nuanced errors.",
    "citations": ["RP-006", "RP-008"],
    "keywords": ["citation hallucination", "context window", "memory compression", "replanning"],
    "stance": "mixed"
  },
  {
    "paper_id": "RP-010",
    "title": "Structured Parsing and Graph Retrieval for PDF-Centric Reviews",
    "year": 2024,
    "language": "en",
    "venue": "AAAI",
    "authors": ["Irene Volkov", "Mateo Diaz", "Jiawen Sun"],
    "abstract": "This paper presents a structured parser that emits section graphs from PDF papers. The graph improves retrieval recall by preserving citation and section topology.",
    "methodology": "The model is tested against flat text chunking and sparse keyword indexing. They run downstream review tasks with multilingual and contradictory query sets.",
    "findings": "Graph-aware hybrid retrieval improves section-level precision and supports better cross-paper synthesis under conflicting evidence.",
    "limitations": "The parser requires domain adaptation for non-STEM documents and legal-style PDFs.",
    "citations": ["RP-008"],
    "keywords": ["structured parsing", "graph retrieval", "PDF", "cross-paper synthesis"],
    "stance": "supports"
  }
]

